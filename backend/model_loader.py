import torch
import numpy as np
import regex as re
import string
from transformers import AutoTokenizer, AutoModelForSequenceClassification

MODEL_PATH = "./model" 

# ƒê·ªãnh nghƒ©a nh√£n (Kh·ªõp v·ªõi uitnlp dataset)
LABELS = {
    0: "Ti√™u c·ª±c üò°",
    1: "Trung t√≠nh üòê",
    2: "T√≠ch c·ª±c üòÉ"
}

emoji_pattern = re.compile("["
                u"\U0001F600-\U0001F64F"
                u"\U0001F300-\U0001F5FF"
                u"\U0001F680-\U0001F6FF"
                u"\U0001F1E0-\U0001F1FF"
                u"\U00002702-\U000027B0"
                u"\U000024C2-\U0001F251"
                u"\U0001f926-\U0001f937"
                u'\U00010000-\U0010ffff'
                u"\u200d"
                u"\u2640-\u2642"
                u"\u2600-\u2B55"
                u"\u23cf"
                u"\u23e9"
                u"\u231a"
                u"\u3030"
                u"\ufe0f"
    "]+", flags=re.UNICODE)
class SentimentModel:
    def __init__(self):
        print("--- KH·ªûI T·∫†O PHOBERT ENGINE ---")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: {self.device}")

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
            self.model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
            self.model.to(self.device)
            self.model.eval()
            print("‚úÖ ƒê√£ load PhoBERT th√†nh c√¥ng!")
        except Exception as e:
            print(f"‚ùå L·ªñI LOAD MODEL: {e}")
            print("H√£y ch·∫Øc ch·∫Øn b·∫°n ƒë√£ copy TO√ÄN B·ªò file trong folder output v√†o backend/model/")

    def clean_text(self,text):

        """
        H√†m l√†m s·∫°ch v√† ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát
        """
        # 1. Chuy·ªÉn to√†n b·ªô vƒÉn b·∫£n v·ªÅ ch·ªØ th∆∞·ªùng
        text = text.lower()

        # 2. Lo·∫°i b·ªè t·∫•t c·∫£ emoji, thay th·∫ø b·∫±ng kho·∫£ng tr·∫Øng
        text = re.sub(emoji_pattern, "[emoji]", text)

        # 3. Gi·∫£m k√Ω t·ª± l·∫∑p li√™n ti·∫øp (v√≠ d·ª•: 'aaabbb' -> 'ab')
        text = re.sub(r"(\p{L})\1{1,}", r"\1", text)

        # 4. ƒê·∫£m b·∫£o c√≥ kho·∫£ng tr·∫Øng tr∆∞·ªõc v√† sau d·∫•u c√¢u
        # Gi·ªØa t·ª´, d·∫•u c√¢u, v√† t·ª´ kh√°c
        text = re.sub(r"(\w)\s*([" + string.punctuation + "])\s*(\w)", r"\1 \2 \3", text)
        # T·ª´ v√† d·∫•u c√¢u ·ªü cu·ªëi
        text = re.sub(r"(\w)\s*([" + string.punctuation + "])", r"\1 \2", text)
        text = re.sub(r"([!?])\1{1,}", r"\1\1\1", text)
        # re.sub(r"[\u200B-\u200D\uFEFF]", "", text)
        # # 5. Gi·∫£m nhi·ªÅu d·∫•u c√¢u li√™n ti·∫øp th√†nh m·ªôt d·∫•u duy nh·∫•t
        text = re.sub(r"([.])\1{3,}", r"\1\1\1", text)
        text = re.sub(r"(?<![.,])([.,]{2})(?![.,])", r"\1", text)

        # 6. Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a ·ªü ƒë·∫ßu v√† cu·ªëi vƒÉn b·∫£n
        text = text.strip()

        # # 7. Lo·∫°i b·ªè d·∫•u c√¢u v√† kho·∫£ng tr·∫Øng th·ª´a ·ªü ƒë·∫ßu vƒÉn b·∫£n
        # while text.startswith(tuple(string.punctuation + string.whitespace)):
        #     text = text[1:]

        # # 8. Lo·∫°i b·ªè d·∫•u c√¢u v√† kho·∫£ng tr·∫Øng th·ª´a ·ªü cu·ªëi vƒÉn b·∫£n
        # while text.endswith(tuple(string.punctuation + string.whitespace)):
        #     text = text[:-1]

        # 10. Gi·∫£m nhi·ªÅu kho·∫£ng tr·∫Øng li√™n ti·∫øp th√†nh m·ªôt kho·∫£ng tr·∫Øng duy nh·∫•t
        text = re.sub(r"\s+", " ", text)

        return text

    def predict(self, text):
        # 1. Ti·ªÅn x·ª≠ l√Ω
        clean_content = self.clean_text(text)
        
        # 2. Tokenize
        inputs = self.tokenizer(
            clean_content, 
            return_tensors="pt", 
            truncation=True, 
            padding=True, 
            max_length=128
        ).to(self.device)

        input_ids = inputs["input_ids"][0]
        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)

        # 3. D·ª± ƒëo√°n (B·∫≠t c·∫£ hidden_states V√Ä attentions)
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True, output_attentions=True)
            logits = outputs.logits
            
            # A. L·∫§Y VECTOR EMBEDDING (Nh∆∞ c≈©)
            last_hidden_state = outputs.hidden_states[-1]
            embeddings = last_hidden_state[0].cpu().numpy().tolist()

            # B. L·∫§Y ATTENTION TH·∫¨T (M·ªöI)
            # outputs.attentions l√† tuple g·ªìm 12 layers. Ta l·∫•y layer cu·ªëi c√πng [-1]
            # Shape: [batch_size, num_heads, seq_len, seq_len] -> [1, 12, N, N]


            # print("hehe", outputs.attentions[-1])

            last_layer_attn = outputs.attentions[-1] 
            
            # T√≠nh trung b√¨nh c·ªông c·ªßa 12 heads ƒë·ªÉ ra 1 ma tr·∫≠n t·ªïng qu√°t [1, N, N]
            avg_attn = torch.mean(last_layer_attn, dim=1)[0] # Shape: [N, N]
            
            # ƒê·ªÉ hi·ªÉn th·ªã l√™n bi·ªÉu ƒë·ªì d·∫°ng thanh ƒë∆°n gi·∫£n (1 chi·ªÅu), ta s·∫Ω l·∫•y gi√° tr·ªã MAX c·ªßa m·ªói h√†ng
            # √ù nghƒ©a: "T·ª´ n√†y t·∫≠p trung m·∫°nh nh·∫•t v√†o ƒë√¢u?"
            attn_scores = torch.max(avg_attn, dim=1).values.cpu().numpy().tolist()
            
            # (Ho·∫∑c n·∫øu b·∫°n mu·ªën l·∫•y ƒë∆∞·ªùng ch√©o - self attention):
            # attn_scores = torch.diagonal(avg_attn, 0).cpu().numpy().tolist()

        # 4. T√≠nh x√°c su·∫•t
        probs = torch.nn.functional.softmax(logits, dim=-1)
        probs = probs.cpu().numpy()[0]

        pred_label_idx = np.argmax(probs)
        pred_label = LABELS.get(pred_label_idx, "Unknown")
        confidence = float(probs[pred_label_idx])

        return {
            "label": pred_label,
            "confidence": round(confidence * 100, 2),
            "original_text": text,
            "tokens": tokens,
            "embeddings": embeddings, 
            "attentions": attn_scores, # <--- G·ª≠i ƒëi·ªÉm Attention th·∫≠t v·ªÅ
            "probs": {
                "positive": float(probs[2]),
                "negative": float(probs[0]),
                "neutral": float(probs[1]) 
            }
        }
ai_engine = SentimentModel()